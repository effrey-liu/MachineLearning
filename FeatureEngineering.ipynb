{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 字典抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.0\n",
      "  (0, 3)\t5.0\n",
      "  (1, 0)\t1.0\n",
      "  (1, 3)\t5.9\n",
      "  (2, 2)\t1.0\n",
      "  (2, 3)\t9.9\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.  1.  0.  5. ]\n",
      " [1.  0.  0.  5.9]\n",
      " [0.  0.  1.  9.9]]\n",
      "['fruit=橘子', 'fruit=苹果', 'fruit=菠萝', 'price']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "fruit = [{\"fruit\": \"苹果\" ,\"price\": 5}, {\"fruit\": \"橘子\" ,\"price\": 5.9}, {\"fruit\": \"菠萝\" ,\"price\": 9.9}]\n",
    "\"\"\"\n",
    "特征抽取——字典抽取\n",
    "\n",
    "\"\"\"\n",
    "vect = DictVectorizer()\n",
    "result = vect.fit_transform(fruit)\n",
    "\n",
    "print( result )\n",
    "print( type(result) )\n",
    "print( result.toarray() )\n",
    "print( vect.get_feature_names() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本抽取\n",
    "## 英文文本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 2)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 10)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 4)\t1\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 1 1 0 0 1 1 0 0 0 0 0]\n",
      " [1 0 1 1 1 1 0 0 1 1 1 1 1]]\n",
      "['always', 'doubleone', 'is', 'life', 'lin', 'long', 'need', 'short', 'so', 'stay', 'to', 'want', 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\"\n",
    "文本特征抽取——CountVectorizer\n",
    "\"\"\"\n",
    "\n",
    "vect = CountVectorizer()\n",
    "result = vect.fit_transform([\"life is short, I need Doubleone\",\"life is so long, I want always to stay with Lin\"])\n",
    "\n",
    "print( result )\n",
    "print( type(result) )\n",
    "print( result.toarray() )\n",
    "print( vect.get_feature_names() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 中文文本抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 82)\t1\n",
      "  (0, 35)\t1\n",
      "  (0, 72)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 23)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 91)\t1\n",
      "  (0, 94)\t1\n",
      "  (0, 87)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 69)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 34)\t1\n",
      "  (0, 42)\t1\n",
      "  (0, 66)\t1\n",
      "  (0, 85)\t1\n",
      "  (0, 54)\t1\n",
      "  (0, 24)\t2\n",
      "  (0, 74)\t1\n",
      "  (0, 60)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 73)\t1\n",
      "  (0, 52)\t2\n",
      "  :\t:\n",
      "  (1, 55)\t1\n",
      "  (1, 39)\t1\n",
      "  (1, 44)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 40)\t1\n",
      "  (2, 61)\t1\n",
      "  (2, 68)\t1\n",
      "  (2, 51)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 46)\t1\n",
      "  (2, 98)\t1\n",
      "  (2, 101)\t1\n",
      "  (2, 37)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 58)\t1\n",
      "  (2, 10)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 18)\t1\n",
      "  (2, 57)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 9)\t1\n",
      "  (2, 63)\t1\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 0 1 1 0 1 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 1 2 1 1 1 1 0 1 1 1 1 1 1\n",
      "  1 0 1 0 0 1 1 1 0 1 0 1 1 0 1 0 2 1 1 0 1 0 0 0 1 0 1 0 0 0 1 1 0 1 2 1\n",
      "  1 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0 0 2 0 0 0 0 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0]]\n",
      "['一个', '一切', '一年', '一段', '一泻千里', '一生', '一股', '一路上', '一间', '不够', '不该', '丰富', '之前', '之后', '享受', '人人', '似太长', '低低的', '作为', '做之事', '催逼', '内在', '冬月', '冰雪', '冲倒', '前波', '前身', '前阻', '勇敢', '匆匆', '危崖', '发源', '合成', '后浪', '向下', '向东流', '吟唱', '四十岁', '回旋', '圣诞', '太短', '夹岸', '奔注', '奔腾', '小屋', '巉岩', '已经', '平沙', '度过', '当中', '心平气和', '心愿', '快乐', '怒吼', '悬崖峭壁', '愉快', '愤激', '所以', '所有', '拥有', '挟卷', '搞清楚', '斜阳', '时间', '明亮', '晚上', '曲折', '最高处', '有何', '有力', '有时候', '桃花', '江春水', '沙石', '沙积土', '洪涛', '流着', '流走', '浪漫', '温暖', '滚滚', '漫天', '生命', '直到', '看见', '穿过', '红艳', '细流', '细细的', '经过', '羞怯', '聚集', '芳草', '行程', '许多', '起伏', '起来', '轻轻地', '过去', '遇到', '遭遇', '那么', '静静地']\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "string1 = \"生命像向东流的一江春水，他从最高处发源，冰雪是他的前身。他聚集起许多细流，合成一股有力的洪涛，向下奔注，他曲折的穿过了悬崖峭壁，冲倒了层沙积土，挟卷着滚滚的沙石，快乐勇敢地流走，一路上他享受着他所遭遇的一切：有时候他遇到巉岩前阻，()他愤激地奔腾了起来，怒吼着，回旋着，前波后浪的起伏催逼，直到他过了，冲倒了这危崖他才心平气和的一泻千里。有时候他经过了细细的平沙，斜阳芳草里，看见了夹岸红艳的桃花，他快乐而又羞怯，静静地流着，低低的吟唱着，轻轻地度过这一段浪漫的行程。\"\n",
    "string2 = \"一个拥有丰富内在的人，就像在冬月的晚上，在漫天冰雪当中拥有一间明亮、温暖、愉快的圣诞小屋。\"\n",
    "string3 = \"一生像似太长，却又太短，待搞清楚有何心愿，一年已经过去，那么四十岁之前若不匆匆把所有该做或不该做之事做妥，之后也无甚作为，所以人人不够时间\"\n",
    "\n",
    "\"\"\"\n",
    "punct = set()\n",
    "filterpunct = lambda s:''.join(filter(lambda x: x not in punct,s))\n",
    "\n",
    "words1 = \" \".join(jieba.cut(filterpunct(string1)))\n",
    "\"\"\"\n",
    "\n",
    "words1 = \" \".join(jieba.cut(string1))\n",
    "words2 = \" \".join(jieba.cut(string2))\n",
    "words3 = \" \".join(jieba.cut(string3))\n",
    "# list(words1)\n",
    "\n",
    "vect = CountVectorizer()\n",
    "result = vect.fit_transform([words1,words2,words3])\n",
    "\n",
    "print( result )\n",
    "print( type(result) )\n",
    "print( result.toarray() )\n",
    "print( vect.get_feature_names() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF抽取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 93)\t0.11502736062564178\n",
      "  (0, 78)\t0.11502736062564178\n",
      "  (0, 3)\t0.11502736062564178\n",
      "  (0, 48)\t0.11502736062564178\n",
      "  (0, 97)\t0.11502736062564178\n",
      "  (0, 36)\t0.11502736062564178\n",
      "  (0, 17)\t0.11502736062564178\n",
      "  (0, 76)\t0.11502736062564178\n",
      "  (0, 102)\t0.11502736062564178\n",
      "  (0, 90)\t0.11502736062564178\n",
      "  (0, 71)\t0.11502736062564178\n",
      "  (0, 86)\t0.11502736062564178\n",
      "  (0, 41)\t0.11502736062564178\n",
      "  (0, 84)\t0.11502736062564178\n",
      "  (0, 92)\t0.11502736062564178\n",
      "  (0, 62)\t0.11502736062564178\n",
      "  (0, 47)\t0.11502736062564178\n",
      "  (0, 88)\t0.11502736062564178\n",
      "  (0, 89)\t0.11502736062564178\n",
      "  (0, 4)\t0.11502736062564178\n",
      "  (0, 50)\t0.11502736062564178\n",
      "  (0, 30)\t0.11502736062564178\n",
      "  (0, 83)\t0.11502736062564178\n",
      "  (0, 20)\t0.11502736062564178\n",
      "  (0, 95)\t0.11502736062564178\n",
      "  :\t:\n",
      "  (1, 59)\t0.47702411121907506\n",
      "  (1, 0)\t0.23851205560953753\n",
      "  (1, 23)\t0.18139456604738438\n",
      "  (2, 63)\t0.2132007163556104\n",
      "  (2, 9)\t0.2132007163556104\n",
      "  (2, 15)\t0.2132007163556104\n",
      "  (2, 57)\t0.2132007163556104\n",
      "  (2, 18)\t0.2132007163556104\n",
      "  (2, 13)\t0.2132007163556104\n",
      "  (2, 19)\t0.2132007163556104\n",
      "  (2, 10)\t0.2132007163556104\n",
      "  (2, 58)\t0.2132007163556104\n",
      "  (2, 29)\t0.2132007163556104\n",
      "  (2, 12)\t0.2132007163556104\n",
      "  (2, 37)\t0.2132007163556104\n",
      "  (2, 101)\t0.2132007163556104\n",
      "  (2, 98)\t0.2132007163556104\n",
      "  (2, 46)\t0.2132007163556104\n",
      "  (2, 2)\t0.2132007163556104\n",
      "  (2, 51)\t0.2132007163556104\n",
      "  (2, 68)\t0.2132007163556104\n",
      "  (2, 61)\t0.2132007163556104\n",
      "  (2, 40)\t0.2132007163556104\n",
      "  (2, 16)\t0.2132007163556104\n",
      "  (2, 5)\t0.2132007163556104\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0.         0.11502736 0.         0.11502736 0.11502736 0.\n",
      "  0.11502736 0.11502736 0.         0.         0.         0.\n",
      "  0.         0.         0.11502736 0.         0.         0.11502736\n",
      "  0.         0.         0.11502736 0.         0.         0.08748127\n",
      "  0.23005472 0.11502736 0.11502736 0.11502736 0.11502736 0.\n",
      "  0.11502736 0.11502736 0.11502736 0.11502736 0.11502736 0.11502736\n",
      "  0.11502736 0.         0.11502736 0.         0.         0.11502736\n",
      "  0.11502736 0.11502736 0.         0.11502736 0.         0.11502736\n",
      "  0.11502736 0.         0.11502736 0.         0.23005472 0.11502736\n",
      "  0.11502736 0.         0.11502736 0.         0.         0.\n",
      "  0.11502736 0.         0.11502736 0.         0.         0.\n",
      "  0.11502736 0.11502736 0.         0.11502736 0.23005472 0.11502736\n",
      "  0.11502736 0.11502736 0.11502736 0.11502736 0.11502736 0.11502736\n",
      "  0.11502736 0.         0.11502736 0.         0.11502736 0.11502736\n",
      "  0.11502736 0.11502736 0.11502736 0.11502736 0.11502736 0.11502736\n",
      "  0.11502736 0.11502736 0.11502736 0.11502736 0.11502736 0.11502736\n",
      "  0.11502736 0.11502736 0.         0.11502736 0.11502736 0.\n",
      "  0.11502736]\n",
      " [0.23851206 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.23851206 0.         0.         0.23851206\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23851206 0.23851206 0.18139457\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.23851206 0.         0.\n",
      "  0.         0.         0.23851206 0.         0.         0.\n",
      "  0.         0.23851206 0.         0.         0.         0.\n",
      "  0.         0.23851206 0.         0.         0.         0.47702411\n",
      "  0.         0.         0.         0.         0.23851206 0.23851206\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.23851206 0.         0.23851206 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.21320072 0.         0.         0.21320072\n",
      "  0.         0.         0.         0.21320072 0.21320072 0.\n",
      "  0.21320072 0.21320072 0.         0.21320072 0.21320072 0.\n",
      "  0.21320072 0.21320072 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.21320072\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.21320072 0.         0.         0.21320072 0.\n",
      "  0.         0.         0.         0.         0.21320072 0.\n",
      "  0.         0.         0.         0.21320072 0.         0.\n",
      "  0.         0.         0.         0.21320072 0.21320072 0.\n",
      "  0.         0.21320072 0.         0.21320072 0.         0.\n",
      "  0.         0.         0.21320072 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.21320072 0.         0.         0.21320072\n",
      "  0.        ]]\n",
      "['一个', '一切', '一年', '一段', '一泻千里', '一生', '一股', '一路上', '一间', '不够', '不该', '丰富', '之前', '之后', '享受', '人人', '似太长', '低低的', '作为', '做之事', '催逼', '内在', '冬月', '冰雪', '冲倒', '前波', '前身', '前阻', '勇敢', '匆匆', '危崖', '发源', '合成', '后浪', '向下', '向东流', '吟唱', '四十岁', '回旋', '圣诞', '太短', '夹岸', '奔注', '奔腾', '小屋', '巉岩', '已经', '平沙', '度过', '当中', '心平气和', '心愿', '快乐', '怒吼', '悬崖峭壁', '愉快', '愤激', '所以', '所有', '拥有', '挟卷', '搞清楚', '斜阳', '时间', '明亮', '晚上', '曲折', '最高处', '有何', '有力', '有时候', '桃花', '江春水', '沙石', '沙积土', '洪涛', '流着', '流走', '浪漫', '温暖', '滚滚', '漫天', '生命', '直到', '看见', '穿过', '红艳', '细流', '细细的', '经过', '羞怯', '聚集', '芳草', '行程', '许多', '起伏', '起来', '轻轻地', '过去', '遇到', '遭遇', '那么', '静静地']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "\n",
    "result =tf.fit_transform([words1,words2,words3])\n",
    "\n",
    "print( result )\n",
    "print( type(result) )\n",
    "print( result.toarray() )\n",
    "print( vect.get_feature_names() )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
